{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e522b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf203b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUB200Dataset(Dataset):\n",
    "    def __init__(self, images_tensor, img_id_to_class_id, train_val_img_ids, class_id_to_class_name, transform=None):\n",
    "        self.images = images_tensor\n",
    "        self.labels = [img_id_to_class_id[train_val_img_ids[i]] for i in range(len(images_tensor))]\n",
    "        self.label_names = [class_id_to_class_name[img_id_to_class_id[train_val_img_ids[i]]] for i in range(len(images_tensor))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]  # شکل (C, H, W) مثلاً (3, 64, 64)\n",
    "        label = self.labels[idx]  # class id\n",
    "        label_name = self.label_names[idx]\n",
    "        \n",
    "        # تبدیل به PIL برای اعمال transform\n",
    "        image = transforms.ToPILImage()(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label, label_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f631313",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # مقادیر استاندارد ImageNet\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226fd0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_images = torch.load('dataset/imgs_train_val_64x64.pth')\n",
    "\n",
    "with torch.serialization.safe_globals([frozenset]):\n",
    "    data_labels = torch.load(\"dataset/metadata.pth\", weights_only=True)\n",
    "\n",
    "img_ids = data_labels['train_val_img_ids']\n",
    "num_total = len(img_ids)\n",
    "num_train = int(0.8 * num_total)\n",
    "num_val = num_total - num_train\n",
    "\n",
    "img_id_to_class_id = data_labels['img_id_to_class_id']\n",
    "class_id_to_class_name = data_labels['class_id_to_class_name']\n",
    "dataset = CUB200Dataset(data_images, \n",
    "                        img_id_to_class_id,\n",
    "                        img_ids,\n",
    "                        class_id_to_class_name,\n",
    "                        transform=resnet_transform)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75abc16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels, _ in dataloader:\n",
    "    print(\"Image batch shape:\", images.shape)  # [B, 3, 224, 224]\n",
    "    print(\"Label batch shape:\", labels.shape)  # [B]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a8824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# برای معکوس کردن نرمال‌سازی ImageNet\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "\n",
    "def imshow(img, title=None):\n",
    "    img = inv_normalize(img)  # برگردوندن نرمال‌سازی\n",
    "    img = torch.clamp(img, 0, 1)  # مقادیر رو بین 0 و 1 نگه می‌داریم\n",
    "    npimg = img.permute(1, 2, 0).numpy()\n",
    "    plt.imshow(npimg)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def show_images_in_row(images, labels=None, class_names=None, num_images=5):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(num_images):\n",
    "        img = inv_normalize(images[i])\n",
    "        img = torch.clamp(img, 0, 1)\n",
    "        npimg = img.permute(1, 2, 0).numpy()\n",
    "\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(npimg)\n",
    "        if labels is not None:\n",
    "            label = labels[i].item()\n",
    "            plt.title(f\"Label: {class_names[i]}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4870494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataiter = iter(dataloader)\n",
    "images, labels, labels_name = next(dataiter)\n",
    "show_images_in_row(images, labels, class_names=labels_name, num_images=5)\n",
    "print(labels_name[:5])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce87743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = data_labels['num_classes']\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3999d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 120\n",
    "# lbl_tensor = data_labels['class_id_to_class_name'][data_labels['img_id_to_class_id'][data_labels['img_ids'][idx]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ed1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch.nn as nn\n",
    "\n",
    "resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "layers =  list(resnet.children())[:-2]\n",
    "model = nn.Sequential(*layers)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d90d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmap_size = 8\n",
    "# features = 512\n",
    "\n",
    "# class Cnn_Resnet_Without_Attention(nn.Module):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.feature = model\n",
    "#         self.dropout = nn.Dropout(.5)\n",
    "#         self.fc = nn.Linear(features, num_classes)\n",
    "#         nn.init.xavier_normal_(self.fc.weight.data)\n",
    "#         if self.fc.bias is not None:\n",
    "#             torch.nn.init.constant_(self.fc.bias.data, val=0)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         bs = x.size()[0]\n",
    "\n",
    "#         x = self.feature(x)\n",
    "#         x = x.view(bs, features, fmap_size ** 2)\n",
    "#         # Batch matrix multiplication\n",
    "#         x = torch.bmm(x, torch.transpose(x, 1, 2))/ (fmap_size ** 2)\n",
    "#         x = torch.sqrt(x + 1e-5)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea4b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import FloatTensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def new_parameter(*size):\n",
    "    out = nn.Parameter(FloatTensor(*size), requires_grad=True)\n",
    "    torch.nn.init.xavier_normal_(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, attention_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = new_parameter(attention_size, 1)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        # after this, we have (bs, feature_size, feature_size) with a diff weight per each cell\n",
    "        attention_score = torch.matmul(x_in, self.attention).squeeze()\n",
    "        attention_score = F.softmax(attention_score, dim=-1).view(x_in.size(0), x_in.size(1), 1)\n",
    "        scored_x = x_in * attention_score\n",
    "\n",
    "        # now, sum across dim 1 to get the expected feature vector\n",
    "        condensed_x = torch.sum(scored_x, dim=1)\n",
    "\n",
    "        return condensed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap_size = 7\n",
    "features = 2048\n",
    "\n",
    "class Cnn_Resnet(nn.Module):\n",
    "    def __init__(self, attention = False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.attention = attention\n",
    "        self.feature = model\n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        self.fc = nn.Linear(features, num_classes)\n",
    "        self.attn = Attention(features)\n",
    "        nn.init.xavier_normal_(self.fc.weight.data)\n",
    "        if self.fc.bias is not None:\n",
    "            torch.nn.init.constant_(self.fc.bias.data, val=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs = x.size()[0]\n",
    "\n",
    "        x = self.feature(x)\n",
    "        x = x.view(bs, features, fmap_size ** 2)\n",
    "        # Batch matrix multiplication\n",
    "        x = torch.bmm(x, torch.transpose(x, 1, 2))/ (fmap_size ** 2)\n",
    "        x = torch.sqrt(x + 1e-5)\n",
    "        \n",
    "        if(self.attention == True):\n",
    "            x = self.attn(x)\n",
    "        print('x shape:', x.shape)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af66683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Train the Cnn_Resnet_With_Attention model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (DataLoader): DataLoader for the training data.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        device (torch.device): Device to train on (CPU or GPU).\n",
    "        num_epochs (int): Number of epochs to train for.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        for images, labels, _ in dataloader:\n",
    "            # Move data to the specified device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate running loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "        # Print epoch statistics\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        epoch_accuracy = correct_predictions / total_predictions * 100\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aa8573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train(model, dataloader, criterion, optimizer, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5810feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_without_attention = Cnn_Resnet()\n",
    "cnn_model_without_attention.to(device)\n",
    "run_training(cnn_model_without_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a1b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_with_attention = Cnn_Resnet(attention=True)\n",
    "cnn_model_with_attention.to(device)\n",
    "run_training(cnn_model_with_attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
